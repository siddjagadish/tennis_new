{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we'll try to fit our very first model on TennisExplorer data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/siddhantjagadish/Documents/DataProjects/tennis_new/venv/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3214: DtypeWarning: Columns (0) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  if (yield from self.run_code(code, result)):\n"
     ]
    }
   ],
   "source": [
    "from tennis_new.fetch.tennis_explorer.combiner import read_joined\n",
    "\n",
    "jd = read_joined()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(952932, 31)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jd.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filters \n",
    "def missing_score_error(df):\n",
    "    return (\n",
    "        df['p1_sets_won'].isnull() |\n",
    "        df['p2_sets_won'].isnull()\n",
    "    )\n",
    "\n",
    "def possible_walkover(df):\n",
    "    return (\n",
    "        (df['p1_sets_won'] == 1) &\n",
    "        df['p1_set1'].isnull()\n",
    "    )\n",
    "\n",
    "def retirement(df):    \n",
    "    return (\n",
    "        (df['p1_sets_won'] == 1) &\n",
    "        df['p1_set1'].notnull()\n",
    "    )\n",
    "\n",
    "def missing_pids(df):    \n",
    "    return (\n",
    "        df['p1_link'].isnull() | \n",
    "        df['p2_link'].isnull()\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Back to Fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit only when we have the unique identifier for both players?\n",
    "bad_filter = missing_pids(jd) | missing_score_error(jd)\n",
    "rel = jd[~bad_filter]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tennis_new.ml.elo import ELOModel\n",
    "\n",
    "unfiltered_elo = ELOModel(winner_mod=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Automatic pdb calling has been turned ON\n"
     ]
    }
   ],
   "source": [
    "%pdb\n",
    "unfiltered_elo.fit_and_backfill(\n",
    "    rel['p1_link'],\n",
    "    rel['p2_link'],\n",
    "    rel['match_link']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def get_test_set(df, test_min='2011-01-01', test_max='2015-01-01', test_surface=None, filter_walkovers=True):\n",
    "    date_cond = (\n",
    "        (df['date'] >= test_min) &\n",
    "        (df['date'] < test_max)\n",
    "    )\n",
    "    if test_surface is None:\n",
    "        surface_cond = True\n",
    "    cond = date_cond & surface_cond\n",
    "    if filter_walkovers:\n",
    "        cond &= (~possible_walkover(df))\n",
    "    return df[cond]\n",
    "\n",
    "def eval_mod(mod, df, test_min='2011-01-01', test_max='2015-01-01', test_surface=None, filter_walkovers=False):\n",
    "    # TODO: Filter out walkovers from test set\n",
    "    history_df = pd.DataFrame(mod.history)\n",
    "    test_set = get_test_set(\n",
    "        df,\n",
    "        test_min=test_min,\n",
    "        test_max=test_max,\n",
    "        test_surface=test_surface,\n",
    "        filter_walkovers=filter_walkovers\n",
    "    )\n",
    "    test_set = pd.merge(test_set, history_df, left_on='match_link', right_on='match_id')\n",
    "    print(test_set.shape[0])\n",
    "    \n",
    "    accuracy = (test_set['elo_match_prediction'] > 0.5).mean()\n",
    "    w_odds = test_set[\n",
    "        test_set['p1_odds'].notnull() &\n",
    "        test_set['p2_odds'].notnull() &\n",
    "        (test_set['p1_odds'] != test_set['p2_odds'])\n",
    "    ]\n",
    "    n_w_odds = w_odds.shape[0]\n",
    "    odds_accuracy = (w_odds['p1_odds'] < w_odds['p2_odds']).mean()\n",
    "    mod_odds_accuracy = (w_odds['elo_match_prediction'] > 0.5).mean()\n",
    "    return {\n",
    "        'overall_accuracy': accuracy,\n",
    "        'odds_accuracy': odds_accuracy,\n",
    "        'model_odds_accuracy': mod_odds_accuracy,\n",
    "        'n_w_odds': n_w_odds\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "224662\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'overall_accuracy': 0.7275996830794705,\n",
       " 'odds_accuracy': 0.7200428690759507,\n",
       " 'model_odds_accuracy': 0.7074658387051017,\n",
       " 'n_w_odds': 63449}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unfiltered_eval = eval_mod(unfiltered_elo, rel)\n",
    "unfiltered_eval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Try Initial Set Model\n",
    "\n",
    "Note that this model we will definitely have to tune the ELO parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tennis_new.ml.elo import ELOModel\n",
    "\n",
    "unfiltered_set_elo = ELOModel(winner_mod=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "unfiltered_set_elo.fit_and_backfill(\n",
    "    rel['p1_link'],\n",
    "    rel['p2_link'],\n",
    "    rel['match_link'],\n",
    "    ys=rel[['p1_sets_won', 'p2_sets_won']].values\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "224662\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'overall_accuracy': 0.7358520800135314,\n",
       " 'odds_accuracy': 0.7200428690759507,\n",
       " 'model_odds_accuracy': 0.7080332235338619,\n",
       " 'n_w_odds': 63449}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_mod(unfiltered_set_elo, rel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The set model is working pretty well!  We should tune the ELO parameters again"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Now Try Not Fitting on Matches with Retirement\n",
    "\n",
    "It seems reasonable that we may do better if we fit without training on walkovers and retirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_filter = possible_walkover(rel) | missing_score_error(rel)\n",
    "good_filter = ~bad_filter\n",
    "bad_filter.sum(), good_filter.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try not fitting on filtered out matches\n",
    "filtered_elo = ELOModel(winner_mod=True)\n",
    "filtered_elo.fit_and_backfill(\n",
    "    rel['p1_link'],\n",
    "    rel['p2_link'],\n",
    "    rel['match_link'],\n",
    "    filter_mask=good_filter\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_eval = eval_mod(filtered_elo, rel)\n",
    "filtered_eval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We do marginally better fitting only on the non-walkovers (on the part with odds anyway)...Let's do a hypothesis test here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import chi2_contingency\n",
    "\n",
    "ns1 = int(np.round(unfiltered_eval['model_odds_accuracy'] * unfiltered_eval['n_w_odds']))\n",
    "nf1 = unfiltered_eval['n_w_odds'] - ns1\n",
    "\n",
    "ns2 = int(np.round(filtered_eval['model_odds_accuracy'] * filtered_eval['n_w_odds']))\n",
    "nf2 = filtered_eval['n_w_odds'] - ns2\n",
    "\n",
    "arr = np.array([[ns1, nf1], [ns2, nf2]])\n",
    "chi2, p, _, _ = chi2_contingency(arr)\n",
    "p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overall, the difference is not significant -- we also see that the accuracy decreases overall, so maybe these filters aren't great?  Let's try to filter out all retirements now..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_filter = possible_walkover(rel) | missing_score_error(rel) | retirement(rel)\n",
    "good_filter = ~bad_filter\n",
    "bad_filter.sum(), good_filter.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try not fitting on filtered out matches\n",
    "filtered_elo = ELOModel(winner_mod=True)\n",
    "filtered_elo.fit_and_backfill(\n",
    "    rel['p1_link'],\n",
    "    rel['p2_link'],\n",
    "    rel['match_link'],\n",
    "    filter_mask=good_filter\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_eval = eval_mod(filtered_elo, rel)\n",
    "filtered_eval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is even worse!  For now, we won't do any filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tune ELO Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tennis_new.ml.sobol import generate_sobol_seq, get_range_values\n",
    "\n",
    "MIN_C = 100\n",
    "MAX_C = 500\n",
    "MIN_O = 0\n",
    "MAX_O = 50\n",
    "MIN_S = 0\n",
    "MAX_S = 2\n",
    "\n",
    "\n",
    "sobol_vals = generate_sobol_seq(3, 100, 1)\n",
    "cs = get_range_values(MIN_C, MAX_C, sobol_vals[:, 0])\n",
    "os = get_range_values(MIN_O, MAX_O, sobol_vals[:, 1])\n",
    "ss = get_range_values(MIN_S, MAX_S, sobol_vals[:, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.autonotebook import tqdm\n",
    "\n",
    "out = []\n",
    "for c, o, s in tqdm(zip(cs, os, ss)):\n",
    "    cur_elo = ELOModel(c=c, o=o, s=s, winner_mod=True)\n",
    "    cur_elo.fit_and_backfill(\n",
    "        rel['p1_link'],\n",
    "        rel['p2_link'],\n",
    "        rel['match_link'],\n",
    "    )\n",
    "    cur_eval = eval_mod(cur_elo, rel)\n",
    "    cur_eval.update({'c': c, 'o': o, 's': s})\n",
    "    out.append(cur_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tune_df = pd.DataFrame(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tune_df.sort_values('model_odds_accuracy', ascending=False, inplace=True)\n",
    "tune_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def _plot_params(c, o, s):\n",
    "    _x = np.arange(100)\n",
    "    _y = c / (_x + o) ** s\n",
    "    plt.plot(_x, _y, label='c:%0.2f, o: %0.2f, s:%0.2f' % (c, o, s))\n",
    "        \n",
    "\n",
    "def _plot_row(row):\n",
    "    _plot_params(row['c'], row['o'], row['s'])\n",
    "\n",
    "for i in range(5):\n",
    "    _plot_row(tune_df.iloc[i])\n",
    "\n",
    "_default_mod = ELOModel()\n",
    "_plot_params(_default_mod.c, _default_mod.o, _default_mod.s)\n",
    "\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above, it looks like the default parameters (those suggested by ESPN) are better than what we've found through tuning."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
