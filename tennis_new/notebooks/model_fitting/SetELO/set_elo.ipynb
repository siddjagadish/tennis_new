{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we'll try to fit our very first model on TennisExplorer data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/siddhantjagadish/Documents/DataProjects/tennis_new/venv/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3214: DtypeWarning: Columns (0) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  if (yield from self.run_code(code, result)):\n"
     ]
    }
   ],
   "source": [
    "from tennis_new.fetch.tennis_explorer.combiner import read_joined\n",
    "\n",
    "jd = read_joined()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tennis_new.model.utils.filters import (\n",
    "    MissingPIDFilter,\n",
    "    MissingScoreFilter,\n",
    "    PossibleWalkoverFilter,\n",
    "    RetirementFilter,\n",
    "    TrainingFilter\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Back to Fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit only when we have the unique identifier for both players?\n",
    "rel = TrainingFilter.filter(jd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tennis_new.ml.elo import ELOModel\n",
    "\n",
    "match_elo = ELOModel(winner_mod=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Automatic pdb calling has been turned OFF\n"
     ]
    }
   ],
   "source": [
    "%pdb\n",
    "match_elo.fit_and_backfill(\n",
    "    rel['p1_link'],\n",
    "    rel['p2_link'],\n",
    "    rel['match_link']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def get_test_set(df, test_min='2011-01-01', test_max='2015-01-01', test_surface=None, filter_walkovers=True):\n",
    "    date_cond = (\n",
    "        (df['date'] >= test_min) &\n",
    "        (df['date'] < test_max)\n",
    "    )\n",
    "    if test_surface is None:\n",
    "        surface_cond = True\n",
    "    cond = date_cond & surface_cond\n",
    "    if filter_walkovers:\n",
    "        cond &= (~possible_walkover(df))\n",
    "    return df[cond]\n",
    "\n",
    "def eval_mod(mod, df, test_min='2011-01-01', test_max='2015-01-01', test_surface=None, filter_walkovers=False):\n",
    "    # TODO: Filter out walkovers from test set\n",
    "    history_df = pd.DataFrame(mod.history)\n",
    "    test_set = get_test_set(\n",
    "        df,\n",
    "        test_min=test_min,\n",
    "        test_max=test_max,\n",
    "        test_surface=test_surface,\n",
    "        filter_walkovers=filter_walkovers\n",
    "    )\n",
    "    test_set = pd.merge(test_set, history_df, left_on='match_link', right_on='match_id')\n",
    "    \n",
    "    accuracy = (test_set['elo_match_prediction'] > 0.5).mean()\n",
    "    w_odds = test_set[\n",
    "        test_set['p1_odds'].notnull() &\n",
    "        test_set['p2_odds'].notnull() &\n",
    "        (test_set['p1_odds'] != test_set['p2_odds'])\n",
    "    ]\n",
    "    n_w_odds = w_odds.shape[0]\n",
    "    odds_accuracy = (w_odds['p1_odds'] < w_odds['p2_odds']).mean()\n",
    "    mod_odds_accuracy = (w_odds['elo_match_prediction'] > 0.5).mean()\n",
    "    return {\n",
    "        'overall_accuracy': accuracy,\n",
    "        'odds_accuracy': odds_accuracy,\n",
    "        'model_odds_accuracy': mod_odds_accuracy,\n",
    "        'n_w_odds': n_w_odds\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "224662\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'overall_accuracy': 0.7275996830794705,\n",
       " 'odds_accuracy': 0.7200428690759507,\n",
       " 'model_odds_accuracy': 0.7074658387051017,\n",
       " 'n_w_odds': 63449}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "match_eval = eval_mod(match_elo, rel)\n",
    "match_eval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Try Initial Set Model\n",
    "\n",
    "Note that this model we will definitely have to tune the ELO parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tennis_new.ml.elo import ELOModel\n",
    "\n",
    "set_elo = ELOModel(winner_mod=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_elo.fit_and_backfill(\n",
    "    rel['p1_link'],\n",
    "    rel['p2_link'],\n",
    "    rel['match_link'],\n",
    "    ys=rel[['p1_sets_won', 'p2_sets_won']].values\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "224662\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'overall_accuracy': 0.7358520800135314,\n",
       " 'odds_accuracy': 0.7200428690759507,\n",
       " 'model_odds_accuracy': 0.7080332235338619,\n",
       " 'n_w_odds': 63449}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_mod(set_elo, rel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The set model is working pretty well!  We should tune the ELO parameters again"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tune ELO Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tennis_new.ml.sobol import generate_sobol_seq, get_range_values\n",
    "\n",
    "MIN_C = 100\n",
    "MAX_C = 500\n",
    "MIN_O = 0\n",
    "MAX_O = 50\n",
    "MIN_S = 0\n",
    "MAX_S = 2\n",
    "\n",
    "\n",
    "sobol_vals = generate_sobol_seq(3, 100, 1)\n",
    "cs = get_range_values(MIN_C, MAX_C, sobol_vals[:, 0])\n",
    "os = get_range_values(MIN_O, MAX_O, sobol_vals[:, 1])\n",
    "ss = get_range_values(MIN_S, MAX_S, sobol_vals[:, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/siddhantjagadish/Documents/DataProjects/tennis_new/venv/lib/python3.7/site-packages/tqdm/autonotebook/__init__.py:18: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  \" (e.g. in jupyter console)\", TqdmExperimentalWarning)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f113a00dbb9445c1b18ce77d46e2f868",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "224662\n",
      "282149\n",
      "224662\n",
      "282149\n",
      "224662\n",
      "282149\n",
      "224662\n",
      "282149\n",
      "224662\n",
      "282149\n",
      "224662\n",
      "282149\n",
      "224662\n",
      "282149\n",
      "224662\n",
      "282149\n",
      "224662\n",
      "282149\n",
      "224662\n",
      "282149\n",
      "224662\n",
      "282149\n",
      "224662\n",
      "282149\n",
      "224662\n",
      "282149\n",
      "224662\n",
      "282149\n",
      "224662\n",
      "282149\n",
      "224662\n",
      "282149\n",
      "224662\n",
      "282149\n",
      "224662\n",
      "282149\n",
      "224662\n",
      "282149\n",
      "224662\n",
      "282149\n",
      "224662\n",
      "282149\n",
      "224662\n",
      "282149\n",
      "224662\n",
      "282149\n",
      "224662\n",
      "282149\n"
     ]
    }
   ],
   "source": [
    "from tqdm.autonotebook import tqdm\n",
    "\n",
    "out = []\n",
    "test_out = []\n",
    "for c, o, s in tqdm(zip(cs, os, ss)):\n",
    "    cur_elo = ELOModel(c=c, o=o, s=s, winner_mod=True)\n",
    "    cur_elo.fit_and_backfill(\n",
    "        rel['p1_link'],\n",
    "        rel['p2_link'],\n",
    "        rel['match_link'],\n",
    "        ys=rel[['p1_sets_won', 'p2_sets_won']].values\n",
    "    )\n",
    "    cur_eval = eval_mod(cur_elo, rel)\n",
    "    test_eval = eval_mod(cur_elo, rel, test_min='2015-01-01', test_max='2021-01-01')\n",
    "    cur_eval.update({'c': c, 'o': o, 's': s})\n",
    "    test_eval.update({'c': c, 'o': o, 's': s})\n",
    "    out.append(cur_eval)\n",
    "    test_out.append(test_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tune_df = pd.DataFrame(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tune_df.sort_values('model_odds_accuracy', ascending=False, inplace=True)\n",
    "tune_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def _plot_params(c, o, s):\n",
    "    _x = np.arange(100)\n",
    "    _y = c / (_x + o) ** s\n",
    "    plt.plot(_x, _y, label='c:%0.2f, o: %0.2f, s:%0.2f' % (c, o, s))\n",
    "        \n",
    "\n",
    "def _plot_row(row):\n",
    "    _plot_params(row['c'], row['o'], row['s'])\n",
    "\n",
    "for i in range(5):\n",
    "    _plot_row(tune_df.iloc[i])\n",
    "\n",
    "_default_mod = ELOModel()\n",
    "_plot_params(_default_mod.c, _default_mod.o, _default_mod.s)\n",
    "\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above, it looks like the default parameters (those suggested by ESPN) are better than what we've found through tuning."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
