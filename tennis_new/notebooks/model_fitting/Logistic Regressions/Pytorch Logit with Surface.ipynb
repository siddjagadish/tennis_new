{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we extend our pytorch logistic regression model to include surface information.  We find that including surface information results in only very modest (barely noticeable, if at all) model improvement, but that the player-specific surface information largely makes sense.  We suspect that we need:\n",
    "* To tune our coefficient-specific regularization (more regularization for surface offsets than main effects)\n",
    "* To get surface data for all matches.\n",
    "\n",
    "This notebook is also no longer in perfect working shape, and will need some editing to make operational again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/siddhantjagadish/Documents/DataProjects/tennis_new/venv_377/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3214: DtypeWarning: Columns (0) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  if (yield from self.run_code(code, result)):\n"
     ]
    }
   ],
   "source": [
    "from tennis_new.fetch.tennis_explorer.combiner import read_joined\n",
    "\n",
    "jd = read_joined()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run Set ELO\n",
    "\n",
    "Run SetELO first so that we have easy access to training set and validation set and all that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tennis_new.model.config.elo.global_set_elo import SetELO\n",
    "\n",
    "set_elo = SetELO()\n",
    "set_elo.run(jd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'DummyFilter_prediction_AUCMetric': 0.8187031847302881,\n",
       " 'DummyFilter_prediction_AccuracyMetric': 0.7358520800135314,\n",
       " 'DummyFilter_prediction_LogLikelihoodMetric': -0.5226366377611569,\n",
       " 'HasOddsFilter_prediction_AUCMetric': 0.7839029874196454,\n",
       " 'HasOddsFilter_prediction_AccuracyMetric': 0.7056423354253945,\n",
       " 'HasOddsFilter_prediction_LogLikelihoodMetric': -0.5594758958654537,\n",
       " 'DummyFilter_odds_implied_probability_AUCMetric': None,\n",
       " 'DummyFilter_odds_implied_probability_AccuracyMetric': None,\n",
       " 'DummyFilter_odds_implied_probability_LogLikelihoodMetric': None,\n",
       " 'HasOddsFilter_odds_implied_probability_AUCMetric': 0.7937506478103871,\n",
       " 'HasOddsFilter_odds_implied_probability_AccuracyMetric': 0.7114980299325661,\n",
       " 'HasOddsFilter_odds_implied_probability_LogLikelihoodMetric': -0.5501844612492598}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set_elo.validation_evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define Logit Training X, y\n",
    "\n",
    "Now we'll need to create a sparse dataset for the logistic regression.  We'll start by making sure we have the right date filtering.  Recall that for ELO models, our training data is the full date range.  We'll have to manually cut the dates for our logit model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(('1997-01-01', '2010-12-31'),\n",
       " ('2011-01-01', '2014-12-31'),\n",
       " ('2015-01-01', '2020-12-21'))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "elo_training_set = set_elo.training_filter.filter_data(jd)\n",
    "elo_validation_set = set_elo.validation_filter.filter_data(set_elo.all_jd)\n",
    "elo_test_set = set_elo.test_filter.filter_data(set_elo.all_jd)\n",
    "logit_training_set = elo_training_set[\n",
    "    elo_training_set['date'] < elo_validation_set['date'].min()\n",
    "].copy()\n",
    "(\n",
    "    (logit_training_set['date'].min(), logit_training_set['date'].max()),\n",
    "    (elo_validation_set['date'].min(), elo_validation_set['date'].max()),\n",
    "    (elo_test_set['date'].min(), elo_test_set['date'].max())\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mess Around with Pytorch DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Player Mapping\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "all_players = pd.concat([\n",
    "    logit_training_set[['p1_link', 'date']].rename(columns={'p1_link': 'pid'}).drop_duplicates('pid', keep='first'),\n",
    "    logit_training_set[['p2_link', 'date']].rename(columns={'p2_link': 'pid'}).drop_duplicates('pid', keep='first')\n",
    "]).sort_values('date', ascending=True)['pid'].drop_duplicates(keep='first')\n",
    "player_map = dict(enumerate(all_players))\n",
    "inv_player_map = {v: k for k, v in player_map.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "SURFACE_LIST = logit_training_set['surface'].dropna().unique()\n",
    "\n",
    "def ohe_surface(df):\n",
    "    return np.array([\n",
    "        (df['surface'] == s).astype(int).values for s in SURFACE_LIST \n",
    "    ]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "torch_val_df = elo_validation_set[\n",
    "    elo_validation_set['p1_link'].isin(logit_training_set['p1_link']) &\n",
    "    elo_validation_set['p2_link'].isin(logit_training_set['p2_link']) &\n",
    "    (elo_validation_set['date'] < '2012-01-01')\n",
    "].copy()\n",
    "\n",
    "def get_torch_set(df):\n",
    "    X = torch.from_numpy(\n",
    "        pd.DataFrame({\n",
    "            'p1_id': df['p1_link'].map(inv_player_map),\n",
    "            'p2_id': df['p2_link'].map(inv_player_map)\n",
    "        }).values\n",
    "    )\n",
    "    s = torch.from_numpy(ohe_surface(df))\n",
    "    w1 = torch.from_numpy(df['p1_sets_won'].values)\n",
    "    w2 = torch.from_numpy(df['p2_sets_won'].values)\n",
    "    return {'X': X, 's': s, 'w1': w1, 'w2': w2}\n",
    "    \n",
    "torch_train = get_torch_set(logit_training_set)\n",
    "torch_val = get_torch_set(torch_val_df)\n",
    "torch_val_w_surface = get_torch_set(\n",
    "    torch_val_df[\n",
    "        torch_val_df['surface'].isin([\n",
    "            'clay',\n",
    "            'hard',\n",
    "            'indoors',\n",
    "            'grass'\n",
    "        ])\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class MyDataSet(torch.utils.data.Dataset):\n",
    "    def __init__(self, X, s, w1, w2):\n",
    "        self.X = X\n",
    "        self.s = s \n",
    "        self.w1 = w1\n",
    "        self.w2 = w2\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            'X': self.X[idx],\n",
    "            's': self.s[idx],\n",
    "            'w1': self.w1[idx],\n",
    "            'w2': self.w2[idx]\n",
    "        }\n",
    "    \n",
    "batch_size = 1024\n",
    "train_ds = MyDataSet(\n",
    "    torch_train['X'],\n",
    "    torch_train['s'],\n",
    "    torch_train['w1'],\n",
    "    torch_train['w2']\n",
    ")\n",
    "train_dl = torch.utils.data.DataLoader(train_ds, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TorchRunner(object):\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        train_dl,\n",
    "        validation_sets=None,\n",
    "        test_set=None,\n",
    "    ):\n",
    "        self.train_dl = train_dl\n",
    "        self.n_epochs = 0\n",
    "        self.epoch_loss = 0\n",
    "        self.validation_sets = validation_sets\n",
    "        if self.validation_sets is None:\n",
    "            self.validation_sets = []\n",
    "        self.test_set = test_set\n",
    "        self.training_initialized=False\n",
    "        # self.model = None\n",
    "        \n",
    "        \n",
    "    def on_epoch_end(self):\n",
    "        # Callback to do at the end of an epoch\n",
    "        pass\n",
    "    \n",
    "    def on_minibatch_end(self):\n",
    "        # Callback to perform at end of a minibatch\n",
    "        pass\n",
    "   \n",
    "    @property\n",
    "    def model_cls(self):\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    @property\n",
    "    def model_kwargs(self):\n",
    "        return {}\n",
    "\n",
    "    @property\n",
    "    def optimizer_cls(self):\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    @property\n",
    "    def optimizer_params_kwargs(self): \n",
    "        return {}\n",
    "    \n",
    "    @property\n",
    "    def loss_criterion(self):\n",
    "        raise NotImplementedError()\n",
    "    \n",
    "    def loss(self, minibatch_data):\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def init_training(self):\n",
    "        # Instantiate model and optimizer\n",
    "        self.training_initialized = True\n",
    "        self.model = self.model_cls(**self.model_kwargs)\n",
    "        self.optimizer = self.optimizer_cls(\n",
    "            self.optimizer_params_kwargs[0],\n",
    "            **self.optimizer_params_kwargs[1]\n",
    "        )\n",
    "        \n",
    "    def train(self, n_epochs):\n",
    "        if not self.training_initialized:\n",
    "            self.init_training()\n",
    "        for epoch in range(n_epochs):\n",
    "            self.epoch_loss = 0\n",
    "            for minibatch_data in self.train_dl:\n",
    "                self.optimizer.zero_grad()\n",
    "                loss = self.loss(minibatch_data)\n",
    "                self.epoch_loss += loss.item()\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                self.on_minibatch_end()\n",
    "            self.n_epochs += 1\n",
    "            self.on_epoch_end()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SurfaceAgnosticLogisticModel(torch.nn.Module):\n",
    "    # Normal Logistic Regression\n",
    "    def __init__(self, n_players):\n",
    "        super(SurfaceAgnosticLogisticModel, self).__init__()\n",
    "        self.main_embedding = torch.nn.Embedding(n_players, 1)\n",
    "        self.surface_embedding = torch.nn.Embedding(n_players, torch_val_surfaces.shape[1])  # TODO: Declare an N_SURFACES\n",
    "        \n",
    "    def forward(self, data):\n",
    "        main_embedded = self.main_embedding(data['X'])\n",
    "        surface_embedded = self.surface_embedding(data['X'])\n",
    "        s0 = torch.mul(surface_embedded[:, 0, :], data['s'])\n",
    "        s1 = torch.mul(surface_embedded[:, 1, :], data['s'])\n",
    "        surface_diff = (s0 - s1).sum(1)\n",
    "        return torch.sigmoid(main_embedded[:, 0, 0] - main_embedded[:, 1, 0])\n",
    "\n",
    "\n",
    "class SurfaceLogisticModel(torch.nn.Module):\n",
    "    # Logit with Surfaces\n",
    "    def __init__(self, n_players):\n",
    "        super(SurfaceLogisticModel, self).__init__()\n",
    "        self.main_embedding = torch.nn.Embedding(n_players, 1)\n",
    "        self.surface_embedding = torch.nn.Embedding(n_players, torch_val_surfaces.shape[1])  # TODO: Declare an N_SURFACES\n",
    "        \n",
    "    def forward(self, data):\n",
    "        main_embedded = self.main_embedding(data['X'])\n",
    "        surface_embedded = self.surface_embedding(data['X'])\n",
    "        s0 = torch.mul(surface_embedded[:, 0, :], data['s'])\n",
    "        s1 = torch.mul(surface_embedded[:, 1, :], data['s'])\n",
    "        surface_diff = (s0 - s1).sum(1)\n",
    "        return torch.sigmoid(main_embedded[:, 0, 0] - main_embedded[:, 1, 0] + surface_diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "INITIAL_LR = 200.\n",
    "\n",
    "\n",
    "class SurfaceLogitFitter(TorchRunner):\n",
    "\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super(SurfaceLogitFitter, self).__init__(*args, **kwargs)\n",
    "        self.last_epoch_loss = 9999999999999\n",
    "        self.lr = INITIAL_LR\n",
    "\n",
    "    @property\n",
    "    def model_cls(self):\n",
    "        return SurfaceLogisticModel \n",
    "\n",
    "    @property\n",
    "    def model_kwargs(self):\n",
    "        return {\n",
    "            'n_players': len(player_map) \n",
    "        }\n",
    "        \n",
    "    @property\n",
    "    def optimizer_cls(self):\n",
    "        return torch.optim.SGD\n",
    "\n",
    "    @property\n",
    "    def optimizer_params_kwargs(self):\n",
    "        params = [\n",
    "            {'params': self.model.main_embedding.parameters(), 'weight_decay': 0.000001},\n",
    "            {'params': self.model.surface_embedding.parameters(), 'weight_decay': 0.00001},\n",
    "        ]\n",
    "        kwargs = {\n",
    "            'lr': INITIAL_LR\n",
    "        }\n",
    "        return params, kwargs\n",
    "\n",
    "    @property\n",
    "    def loss_criterion(self):\n",
    "        return torch.nn.BCELoss(reduction='none')\n",
    "    \n",
    "    def on_epoch_end(self):\n",
    "        print(\"Iteration: {}, Loss: {}\".format(self.n_epochs, self.epoch_loss))\n",
    "        val_accuracies = []\n",
    "        for val_name, val_set in self.validation_sets:\n",
    "            val_preds = self.model(val_set)\n",
    "            accuracy = (val_preds.detach().numpy() > 0.5).mean()\n",
    "            print(\"Val Set: {}, Accuracy: {}\".format(val_name, accuracy))\n",
    "        if self.epoch_loss > self.last_epoch_loss:  # If training loss is getting worse, halve learning rate\n",
    "            self.lr /= 2.\n",
    "            print(\"Reducing learning rate to %0.2f\" % self.lr)\n",
    "            for pg in self.optimizer.param_groups:\n",
    "                pg['lr'] = self.lr\n",
    "        self.last_epoch_loss = self.epoch_loss\n",
    "        \n",
    "    def loss(self, minibatch_data):\n",
    "        # Set-Weighted Loss\n",
    "        outputs = self.model(minibatch_data)\n",
    "        y_1 = torch.from_numpy(np.ones(minibatch_data['X'].shape[0], dtype=np.float32))\n",
    "        y_2 = torch.from_numpy(np.zeros(minibatch_data['X'].shape[0], dtype=np.float32))\n",
    "        loss_1 = torch.mean(torch.mul(minibatch_data['w1'], self.loss_criterion(outputs, y_1)))\n",
    "        loss_2 = torch.mean(torch.mul(minibatch_data['w2'], self.loss_criterion(1. - outputs, y_2)))\n",
    "        loss = loss_1 + loss_2\n",
    "        return loss\n",
    "    \n",
    "class SurfaceAgnosticLogitFitter(SurfaceLogitFitter):\n",
    "\n",
    "    @property\n",
    "    def model_cls(self):\n",
    "        return SurfaceAgnosticLogisticModel\n",
    "        \n",
    "    @property\n",
    "    def optimizer_params_kwargs(self):\n",
    "        params = [\n",
    "            {'params': self.model.main_embedding.parameters()},\n",
    "        ]\n",
    "        kwargs = {\n",
    "            'lr': INITIAL_LR\n",
    "        }\n",
    "        return params, kwargs\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "surface_agnostic_logit_fitter = SurfaceAgnosticLogitFitter(\n",
    "    train_dl, validation_sets=[\n",
    "        ('full_validation', torch_val),\n",
    "        ('surface_validation', torch_val_w_surface)\n",
    "    ]\n",
    ")\n",
    "surface_logit_fitter = SurfaceLogitFitter(\n",
    "    train_dl, validation_sets=[\n",
    "        ('full_validation', torch_val),\n",
    "        ('surface_validation', torch_val_w_surface)\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torch_val_surfaces' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-df460d70a4cb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msurface_agnostic_logit_fitter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-13-94a1647ab651>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, n_epochs)\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_initialized\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit_training\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepoch_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-13-94a1647ab651>\u001b[0m in \u001b[0;36minit_training\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0;31m# Instantiate model and optimizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_initialized\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_cls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m         self.optimizer = self.optimizer_cls(\n\u001b[1;32m     56\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer_params_kwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-14-f1d510b45421>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, n_players)\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSurfaceAgnosticLogisticModel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmain_embedding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEmbedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_players\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msurface_embedding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEmbedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_players\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch_val_surfaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# TODO: Declare an N_SURFACES\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'torch_val_surfaces' is not defined"
     ]
    }
   ],
   "source": [
    "surface_agnostic_logit_fitter.train(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "surface_logit_fitter.train(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_coefs = surface_logit_fitter.model.main_embedding.weight.detach().numpy()[:, 0]\n",
    "surface_coefs = surface_logit_fitter.model.surface_embedding.weight.detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_surface_coef = surface_coefs.mean(axis=1)\n",
    "surface_coefs -= mean_surface_coef.reshape((len(mean_surface_coef), 1))\n",
    "main_coefs += mean_surface_coef"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_df = pd.DataFrame(\n",
    "    surface_coefs,\n",
    "    columns=logit_training_set['surface'].dropna().drop_duplicates()\n",
    ")\n",
    "embedding_df['main_effect'] = main_coefs\n",
    "embedding_df['player_idx'] = range(len(embedding_df))\n",
    "embedding_df['pid'] = embedding_df['player_idx'].map(player_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logit_training_set['surface'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_df.sort_values('main_effect', ascending=False).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "by_surface_df = embedding_df[['pid']].copy()\n",
    "by_surface_df['clay'] = embedding_df['main_effect'] + embedding_df['clay']\n",
    "by_surface_df['hard'] = embedding_df['main_effect'] + embedding_df['hard']\n",
    "by_surface_df['indoors'] = embedding_df['main_effect'] + embedding_df['indoors']\n",
    "by_surface_df['grass'] = embedding_df['main_effect'] + embedding_df['grass']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pcounts = logit_training_set['p1_link'].value_counts()\n",
    "by_surface_df['pcount'] = by_surface_df['pid'].map(pcounts).fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stronk = logit_training_set[\n",
    "    (logit_training_set['p1_link'] == '/player/nadal/') |\n",
    "    (logit_training_set['p2_link'] == '/player/nadal/')\n",
    "][[\n",
    "    'date',\n",
    "    'p1_link',\n",
    "    'p2_link',\n",
    "    'surface'\n",
    "]]\n",
    "stronk[stronk['surface'] == 'grass']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "by_surface_df[\n",
    "    by_surface_df['pcount'] > 20\n",
    "].sort_values('clay', ascending=False).head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "by_surface_df[\n",
    "    by_surface_df['pcount'] > 20\n",
    "].sort_values('hard', ascending=False).head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "by_surface_df[\n",
    "    by_surface_df['pcount'] > 20\n",
    "].sort_values('grass', ascending=False).head(20)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
