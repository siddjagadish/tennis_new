{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we build some wrapper classes around a simple Pytorch Logit model.  We find that we are able to replicate (and even outperform) the sklearn logit model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/siddhantjagadish/Documents/DataProjects/tennis_new/venv_377/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3214: DtypeWarning: Columns (0) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  if (yield from self.run_code(code, result)):\n"
     ]
    }
   ],
   "source": [
    "from tennis_new.fetch.tennis_explorer.combiner import read_joined\n",
    "\n",
    "jd = read_joined()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run Set ELO\n",
    "\n",
    "Run SetELO first so that we have easy access to training set and validation set and all that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tennis_new.model.config.elo.global_set_elo import SetELO\n",
    "\n",
    "set_elo = SetELO()\n",
    "set_elo.run(jd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'DummyFilter_prediction_AUCMetric': 0.8187031847302881,\n",
       " 'DummyFilter_prediction_AccuracyMetric': 0.7358520800135314,\n",
       " 'DummyFilter_prediction_LogLikelihoodMetric': -0.5226366377611569,\n",
       " 'HasOddsFilter_prediction_AUCMetric': 0.7839029874196454,\n",
       " 'HasOddsFilter_prediction_AccuracyMetric': 0.7056423354253945,\n",
       " 'HasOddsFilter_prediction_LogLikelihoodMetric': -0.5594758958654537,\n",
       " 'DummyFilter_odds_implied_probability_AUCMetric': None,\n",
       " 'DummyFilter_odds_implied_probability_AccuracyMetric': None,\n",
       " 'DummyFilter_odds_implied_probability_LogLikelihoodMetric': None,\n",
       " 'HasOddsFilter_odds_implied_probability_AUCMetric': 0.7937506478103871,\n",
       " 'HasOddsFilter_odds_implied_probability_AccuracyMetric': 0.7114980299325661,\n",
       " 'HasOddsFilter_odds_implied_probability_LogLikelihoodMetric': -0.5501844612492598}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set_elo.validation_evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define Logit Training X, y\n",
    "\n",
    "Now we'll need to create a sparse dataset for the logistic regression.  We'll start by making sure we have the right date filtering.  Recall that for ELO models, our training data is the full date range.  We'll have to manually cut the dates for our logit model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(('1997-01-01', '2010-12-31'),\n",
       " ('2011-01-01', '2014-12-31'),\n",
       " ('2015-01-01', '2020-12-21'))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "elo_training_set = set_elo.training_filter.filter_data(jd)\n",
    "elo_validation_set = set_elo.validation_filter.filter_data(set_elo.all_jd)\n",
    "elo_test_set = set_elo.test_filter.filter_data(set_elo.all_jd)\n",
    "logit_training_set = elo_training_set[\n",
    "    elo_training_set['date'] < elo_validation_set['date'].min()\n",
    "].copy()\n",
    "(\n",
    "    (logit_training_set['date'].min(), logit_training_set['date'].max()),\n",
    "    (elo_validation_set['date'].min(), elo_validation_set['date'].max()),\n",
    "    (elo_test_set['date'].min(), elo_test_set['date'].max())\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mess Around with Pytorch DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "all_players = pd.concat([\n",
    "    logit_training_set[['p1_link', 'date']].rename(columns={'p1_link': 'pid'}).drop_duplicates('pid', keep='first'),\n",
    "    logit_training_set[['p2_link', 'date']].rename(columns={'p2_link': 'pid'}).drop_duplicates('pid', keep='first')\n",
    "]).sort_values('date', ascending=True)['pid'].drop_duplicates(keep='first')\n",
    "player_map = dict(enumerate(all_players))\n",
    "inv_player_map = {v: k for k, v in player_map.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch_training_set = logit_training_set[[\n",
    "    'p1_link',\n",
    "    'p2_link',\n",
    "    'p1_sets_won',\n",
    "    'p2_sets_won'\n",
    "]].copy()\n",
    "torch_training_set['p1_id'] = torch_training_set['p1_link'].map(inv_player_map)\n",
    "torch_training_set['p2_id'] = torch_training_set['p2_link'].map(inv_player_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "torch_validation_set = elo_validation_set[\n",
    "    elo_validation_set['p1_link'].isin(torch_training_set['p1_link']) &\n",
    "    elo_validation_set['p2_link'].isin(torch_training_set['p2_link']) &\n",
    "    (elo_validation_set['date'] < '2012-01-01')\n",
    "].copy()\n",
    "\n",
    "torch_val_X = torch.from_numpy(\n",
    "    pd.DataFrame({\n",
    "        'p1_id': torch_validation_set['p1_link'].map(inv_player_map),\n",
    "        'p2_id': torch_validation_set['p2_link'].map(inv_player_map)\n",
    "    }).values\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26545"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate embedding size\n",
    "embedding_size = torch_training_set[['p1_id', 'p2_id']].max().max() + 1\n",
    "embedding_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class MyDataSet(torch.utils.data.Dataset):\n",
    "    def __init__(self, X, w1, w2):\n",
    "        self.X = torch.from_numpy(X.values)\n",
    "        self.w1 = torch.from_numpy(w1.values.astype(np.float32))\n",
    "        self.w2 = torch.from_numpy(w2.values.astype(np.float32))\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.w1[idx], self.w2[idx]\n",
    "\n",
    "batch_size = 1024\n",
    "train_ds = MyDataSet(\n",
    "    torch_training_set[['p1_id', 'p2_id']], \n",
    "    torch_training_set['p1_sets_won'],\n",
    "    torch_training_set['p2_sets_won']\n",
    ")\n",
    "train_dl = torch.utils.data.DataLoader(train_ds, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TorchRunner(object):\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        train_dl,\n",
    "        validation_set=None,\n",
    "        test_set=None,\n",
    "    ):\n",
    "        self.train_dl = train_dl\n",
    "        self.n_epochs = 0\n",
    "        self.epoch_loss = 0\n",
    "        self.validation_set = validation_set\n",
    "        self.test_set = test_set\n",
    "        self.training_initialized=False\n",
    "        self.model = None\n",
    "        \n",
    "        \n",
    "    def on_epoch_end(self):\n",
    "        # Callback to do at the end of an epoch\n",
    "        pass\n",
    "    \n",
    "    def on_minibatch_end(self):\n",
    "        # Callback to perform at end of a minibatch\n",
    "        pass\n",
    "   \n",
    "    @property\n",
    "    def model_cls(self):\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    @property\n",
    "    def model_kwargs(self):\n",
    "        return {}\n",
    "\n",
    "    @property\n",
    "    def optimizer_cls(self):\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    @property\n",
    "    def optimizer_kwargs(self): \n",
    "        return {}\n",
    "\n",
    "    '''\n",
    "    @property\n",
    "    def loss_criterion(self):\n",
    "        raise NotImplementedError()\n",
    "    '''\n",
    "    \n",
    "    def loss(self, minibatch_data):\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def init_training(self):\n",
    "        # Instantiate model and optimizer\n",
    "        self.training_initialized = True\n",
    "        self.model = self.model_cls(**self.model_kwargs)\n",
    "        self.optimizer = self.optimizer_cls(self.model.parameters(), **self.optimizer_kwargs)\n",
    "        \n",
    "    def train(self, n_epochs):\n",
    "        if not self.training_initialized:\n",
    "            self.init_training()\n",
    "        for epoch in range(n_epochs):\n",
    "            self.epoch_loss = 0\n",
    "            for minibatch_data in self.train_dl:\n",
    "                self.optimizer.zero_grad()\n",
    "                loss = self.loss(minibatch_data)\n",
    "                self.epoch_loss += loss.item()\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                self.on_minibatch_end()\n",
    "            self.n_epochs += 1\n",
    "            self.on_epoch_end()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddedLogisticModel(torch.nn.Module):\n",
    "    def __init__(self, n_players):\n",
    "        super(EmbeddedLogisticModel, self).__init__()\n",
    "        self.embedding = torch.nn.Embedding(n_players, 1)\n",
    "        self.sigmoid = torch.nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x)\n",
    "        return self.sigmoid(embedded[:, 0] - embedded[:, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "INITIAL_LR = 200.\n",
    "\n",
    "\n",
    "class MyLogitFitter(TorchRunner):\n",
    "\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super(MyLogitFitter, self).__init__(*args, **kwargs)\n",
    "        self.last_epoch_loss = 9999999999999\n",
    "        self.lr = INITIAL_LR\n",
    "        \n",
    "    @property\n",
    "    def model_cls(self):\n",
    "        return EmbeddedLogisticModel\n",
    "\n",
    "    @property\n",
    "    def model_kwargs(self):\n",
    "        return {\n",
    "            'n_players': embedding_size\n",
    "        }\n",
    "        \n",
    "    @property\n",
    "    def optimizer_cls(self):\n",
    "        return torch.optim.SGD\n",
    "\n",
    "    @property\n",
    "    def optimizer_kwargs(self):\n",
    "        return {'lr': INITIAL_LR}\n",
    "\n",
    "    @property\n",
    "    def loss_criterion(self):\n",
    "        return torch.nn.BCELoss(reduction='none')\n",
    "    \n",
    "    def on_epoch_end(self):\n",
    "        val_preds = self.model(self.validation_set)\n",
    "        accuracy = (val_preds[:, 0].detach().numpy() > 0.5).mean()\n",
    "        print(\"Iteration: {}, Loss: {}, Accuracy: {}.\".format(self.n_epochs, self.epoch_loss, accuracy))\n",
    "        if self.epoch_loss > self.last_epoch_loss:  # If training loss is getting worse, halve learning rate\n",
    "            self.lr /= 2.\n",
    "            print(\"Reducing learning rate to %0.2f\" % self.lr)\n",
    "            for pg in self.optimizer.param_groups:\n",
    "                pg['lr'] = self.lr\n",
    "        self.last_epoch_loss = self.epoch_loss\n",
    "        \n",
    "    def loss(self, minibatch_data):\n",
    "        # Set-Weighted Loss\n",
    "        X, w1, w2 = minibatch_data\n",
    "        outputs = self.model(X)[:, 0]\n",
    "        y_1 = torch.from_numpy(np.ones(X.shape[0], dtype=np.float32))\n",
    "        y_2 = torch.from_numpy(np.zeros(X.shape[0], dtype=np.float32))\n",
    "        loss_1 = torch.mean(torch.mul(w1, self.loss_criterion(outputs, y_1)))\n",
    "        loss_2 = torch.mean(torch.mul(w2, self.loss_criterion(1. - outputs, y_2)))\n",
    "        loss = loss_1 + loss_2\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "logit_fitter = MyLogitFitter(train_dl, validation_set=torch_val_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1, Loss: 644.2845195531845, Accuracy: 0.6670013234153243.\n",
      "Iteration: 2, Loss: 582.9165781736374, Accuracy: 0.6811025418701228.\n",
      "Iteration: 3, Loss: 569.1016620397568, Accuracy: 0.6855975904714097.\n",
      "Iteration: 4, Loss: 562.0199350118637, Accuracy: 0.6934924474056496.\n",
      "Iteration: 5, Loss: 557.941135764122, Accuracy: 0.6956144754255464.\n",
      "Iteration: 6, Loss: 555.3975894451141, Accuracy: 0.6957285629534979.\n",
      "Iteration: 7, Loss: 552.8754503726959, Accuracy: 0.6984666636243326.\n",
      "Iteration: 8, Loss: 551.0330901145935, Accuracy: 0.6993337288367636.\n",
      "Iteration: 9, Loss: 549.9968975782394, Accuracy: 0.7003376990827362.\n",
      "Iteration: 10, Loss: 548.9059181213379, Accuracy: 0.6996303564094373.\n",
      "Iteration: 11, Loss: 548.3309115171432, Accuracy: 0.6982841235796102.\n",
      "Iteration: 12, Loss: 546.7757701873779, Accuracy: 0.7032355222927029.\n",
      "Iteration: 13, Loss: 547.0961427688599, Accuracy: 0.7020033769908274.\n",
      "Reducing learning rate to 100.00\n",
      "Iteration: 14, Loss: 520.3741438388824, Accuracy: 0.706635330625656.\n",
      "Iteration: 15, Loss: 518.2016954421997, Accuracy: 0.710035138958609.\n",
      "Iteration: 16, Loss: 517.8480447530746, Accuracy: 0.7074795783324966.\n",
      "Iteration: 17, Loss: 517.6170688867569, Accuracy: 0.7081869210057956.\n",
      "Iteration: 18, Loss: 517.2876011133194, Accuracy: 0.7064984255921143.\n",
      "Iteration: 19, Loss: 517.2588114738464, Accuracy: 0.706635330625656.\n",
      "Iteration: 20, Loss: 517.4424015283585, Accuracy: 0.7093506137909004.\n",
      "Reducing learning rate to 50.00\n"
     ]
    }
   ],
   "source": [
    "logit_fitter.train(20)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
